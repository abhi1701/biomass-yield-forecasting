{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43a6d61b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Annual Avg Yield</th>\n",
       "      <th>Annual Avg Min Temp</th>\n",
       "      <th>Annual Avg Max Temp</th>\n",
       "      <th>Total Accumulated Rain</th>\n",
       "      <th>Total Accumulated Radiation</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.615000</td>\n",
       "      <td>3.607579</td>\n",
       "      <td>16.450760</td>\n",
       "      <td>4320.03</td>\n",
       "      <td>589529.46</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.580000</td>\n",
       "      <td>2.977617</td>\n",
       "      <td>15.747898</td>\n",
       "      <td>5426.99</td>\n",
       "      <td>762918.99</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.223333</td>\n",
       "      <td>3.016332</td>\n",
       "      <td>15.804588</td>\n",
       "      <td>5420.95</td>\n",
       "      <td>1083330.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.497500</td>\n",
       "      <td>3.607579</td>\n",
       "      <td>16.450760</td>\n",
       "      <td>4320.03</td>\n",
       "      <td>589529.46</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.640000</td>\n",
       "      <td>2.977617</td>\n",
       "      <td>15.747898</td>\n",
       "      <td>5426.99</td>\n",
       "      <td>762918.99</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Annual Avg Yield  Annual Avg Min Temp  Annual Avg Max Temp  \\\n",
       "0          1.615000             3.607579            16.450760   \n",
       "1          1.580000             2.977617            15.747898   \n",
       "2          1.223333             3.016332            15.804588   \n",
       "3          1.497500             3.607579            16.450760   \n",
       "4          1.640000             2.977617            15.747898   \n",
       "\n",
       "   Total Accumulated Rain  Total Accumulated Radiation  Class  \n",
       "0                 4320.03                    589529.46      1  \n",
       "1                 5426.99                    762918.99      1  \n",
       "2                 5420.95                   1083330.58      0  \n",
       "3                 4320.03                    589529.46      1  \n",
       "4                 5426.99                    762918.99      1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "\n",
    "data = pd.read_csv('~/ctgan/data/annualized_SD_p5_std.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13e177df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba\n",
    "from numba import jit\n",
    "\n",
    "\n",
    "samples_out = 10000 # total number of samples/records to generate/synthesize\n",
    "no_stds = .7 # number of standard deviations within which synthesized values must fall\n",
    "#F = data[[\"Total Radiation (W/m^2)\",\"Total Rainfall (mm)\", \"Avg Max Temp (C)\", \"Avg Min Temp (C)\"]] # the input feature matrix\n",
    "number_of_classes = (data['Class'].unique()).size # number of unique classes in input data\n",
    "F = [] # a list of the feature vectors dataframes, one per class\n",
    "\n",
    "@jit(nopython=True)\n",
    "def get_fv_dfs(data):\n",
    "    for class_no in range(number_of_classes):\n",
    "        df = pd.DataFrame(data[data['Class'] == class_no])\n",
    "        F.append(df)\n",
    "        \n",
    "    return F\n",
    "    \n",
    "# new_F = []\n",
    "# for index, entry in enumerate(F):\n",
    "#     total_rad = entry['Total Accumulated Radiation']\n",
    "#     mean_rad = total_rad.mean()\n",
    "#     std_rad = total_rad.std()\n",
    "#     total_rain = entry['Total Accumulated Rain']\n",
    "#     mean_rain = total_rain.mean()\n",
    "#     std_rain = total_rain.std()\n",
    "#     avg_max_temp = entry['Annual Avg Max Temp']\n",
    "#     mean_max_temp = avg_max_temp.mean()\n",
    "#     std_max_temp = avg_max_temp.std()\n",
    "#     avg_min_temp = entry['Annual Avg Min Temp']\n",
    "#     mean_min_temp = avg_min_temp.mean()\n",
    "#     std_min_temp = avg_min_temp.std()\n",
    "\n",
    "#     new_rads = []\n",
    "#     new_rains = []\n",
    "#     new_max_temps = []\n",
    "#     new_min_temps = []\n",
    "\n",
    "#     # calculate potcii: percentage of this class in input\n",
    "#     potcii = (len(entry)/no_records)\n",
    "#     no_records_to_generate = round(potcii * samples_out)\n",
    "\n",
    "#     for i in range(no_records_to_generate):\n",
    "#         new_rad = random.uniform(mean_rad - std_rad*no_stds, mean_rad + std_rad*no_stds)\n",
    "#         new_rads.append(new_rad)\n",
    "\n",
    "#         new_rain = random.uniform(mean_rain - std_rain*no_stds, mean_rain + std_rain*no_stds)\n",
    "#         new_rains.append(new_rain)\n",
    "\n",
    "#         new_max_temp = random.uniform(mean_max_temp - std_max_temp*no_stds, mean_max_temp + std_max_temp*no_stds)\n",
    "#         new_max_temps.append(new_max_temp)\n",
    "\n",
    "#         new_min_temp = random.uniform(mean_min_temp - std_min_temp*no_stds, mean_min_temp + std_min_temp*no_stds)\n",
    "#         new_min_temps.append(new_min_temp)\n",
    "\n",
    "#     concat_rads = pd.concat([total_rad, pd.DataFrame(new_rads)])\n",
    "#     concat_rain = pd.concat([total_rain, pd.DataFrame(new_rains)])\n",
    "#     concat_max_temps = pd.concat([avg_max_temp, pd.DataFrame(new_max_temps)])\n",
    "#     concat_min_temps = pd.concat([avg_min_temp, pd.DataFrame(new_min_temps)])\n",
    "#     new_df = pd.DataFrame()\n",
    "#     new_df['Total Accumulated Radiation'] = concat_rads\n",
    "#     new_df['Total Accumulated Rain'] = concat_rain\n",
    "#     new_df['Annual Avg Max Temp'] = concat_max_temps\n",
    "#     new_df['Annual Avg Min Temp'] = concat_min_temps\n",
    "#     new_df['Class'] = index\n",
    "#     print(index)\n",
    "#     new_F.append(new_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8ed05b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypingError",
     "evalue": "Failed in nopython mode pipeline (step: nopython frontend)\n\u001b[1mUntyped global name 'F':\u001b[0m \u001b[1mCannot type empty list\n\u001b[1m\nFile \"../../../../tmp/ipykernel_2610929/2041346256.py\", line 15:\u001b[0m\n\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n\u001b[0m \n\nThis error may have been caused by the following argument(s):\n- argument 0: \u001b[1mCannot determine Numba type of <class 'pandas.core.frame.DataFrame'>\u001b[0m\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypingError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 72\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# from sdv.tabular import CTGAN\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# model = CTGAN()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m#new_data = synthesize_tabular_data(samples_out, no_stds, F, number_of_classes, len(data.index))\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m#new_data = sits(data)\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m fv_dfs \u001b[38;5;241m=\u001b[39m \u001b[43mget_fv_dfs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ctgan/lib/python3.9/site-packages/numba/core/dispatcher.py:468\u001b[0m, in \u001b[0;36m_DispatcherBase._compile_for_args\u001b[0;34m(self, *args, **kws)\u001b[0m\n\u001b[1;32m    464\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;241m.\u001b[39mrstrip()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mThis error may have been caused \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    465\u001b[0m                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mby the following argument(s):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00margs_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    466\u001b[0m         e\u001b[38;5;241m.\u001b[39mpatch_message(msg)\n\u001b[0;32m--> 468\u001b[0m     \u001b[43merror_rewrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtyping\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mUnsupportedError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;66;03m# Something unsupported is present in the user code, add help info\u001b[39;00m\n\u001b[1;32m    471\u001b[0m     error_rewrite(e, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124munsupported_error\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ctgan/lib/python3.9/site-packages/numba/core/dispatcher.py:409\u001b[0m, in \u001b[0;36m_DispatcherBase._compile_for_args.<locals>.error_rewrite\u001b[0;34m(e, issue_type)\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 409\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mTypingError\u001b[0m: Failed in nopython mode pipeline (step: nopython frontend)\n\u001b[1mUntyped global name 'F':\u001b[0m \u001b[1mCannot type empty list\n\u001b[1m\nFile \"../../../../tmp/ipykernel_2610929/2041346256.py\", line 15:\u001b[0m\n\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n\u001b[0m \n\nThis error may have been caused by the following argument(s):\n- argument 0: \u001b[1mCannot determine Numba type of <class 'pandas.core.frame.DataFrame'>\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# from sdv.tabular import CTGAN\n",
    "\n",
    "# model = CTGAN()\n",
    "# model.fit(data)\n",
    "\n",
    "\n",
    "#def sits(new_F):\n",
    "#     samples_out = 10000 # total number of samples/records to generate/synthesize\n",
    "#     no_stds = .7 # number of standard deviations within which synthesized values must fall\n",
    "#     #F = data[[\"Total Radiation (W/m^2)\",\"Total Rainfall (mm)\", \"Avg Max Temp (C)\", \"Avg Min Temp (C)\"]] # the input feature matrix\n",
    "#     number_of_classes = (data['Class'].unique()).size # number of unique classes in input data\n",
    "#     F = [] # a list of the feature vectors dataframes, one per class\n",
    "#     for class_no in range(number_of_classes):\n",
    "#         df = pd.DataFrame(data[data['Class'] == class_no])\n",
    "#         F.append(df)\n",
    "        \n",
    "    #return synthesize_tabular_data(so = samples_out, no_stds, F, number_of_classes, len(data.index))\n",
    "#     new_F = []\n",
    "#     for index, entry in enumerate(F):\n",
    "#         total_rad = entry['Total Accumulated Radiation']\n",
    "#         mean_rad = total_rad.mean()\n",
    "#         std_rad = total_rad.std()\n",
    "#         total_rain = entry['Total Accumulated Rain']\n",
    "#         mean_rain = total_rain.mean()\n",
    "#         std_rain = total_rain.std()\n",
    "#         avg_max_temp = entry['Annual Avg Max Temp']\n",
    "#         mean_max_temp = avg_max_temp.mean()\n",
    "#         std_max_temp = avg_max_temp.std()\n",
    "#         avg_min_temp = entry['Annual Avg Min Temp']\n",
    "#         mean_min_temp = avg_min_temp.mean()\n",
    "#         std_min_temp = avg_min_temp.std()\n",
    "        \n",
    "#         new_rads = []\n",
    "#         new_rains = []\n",
    "#         new_max_temps = []\n",
    "#         new_min_temps = []\n",
    "        \n",
    "#         # calculate potcii: percentage of this class in input\n",
    "#         potcii = (len(entry)/no_records)\n",
    "#         no_records_to_generate = round(potcii * samples_out)\n",
    "        \n",
    "#         for i in range(no_records_to_generate):\n",
    "#             new_rad = random.uniform(mean_rad - std_rad*no_stds, mean_rad + std_rad*no_stds)\n",
    "#             new_rads.append(new_rad)\n",
    "            \n",
    "#             new_rain = random.uniform(mean_rain - std_rain*no_stds, mean_rain + std_rain*no_stds)\n",
    "#             new_rains.append(new_rain)\n",
    "        \n",
    "#             new_max_temp = random.uniform(mean_max_temp - std_max_temp*no_stds, mean_max_temp + std_max_temp*no_stds)\n",
    "#             new_max_temps.append(new_max_temp)\n",
    "            \n",
    "#             new_min_temp = random.uniform(mean_min_temp - std_min_temp*no_stds, mean_min_temp + std_min_temp*no_stds)\n",
    "#             new_min_temps.append(new_min_temp)\n",
    "            \n",
    "#         concat_rads = pd.concat([total_rad, pd.DataFrame(new_rads)])\n",
    "#         concat_rain = pd.concat([total_rain, pd.DataFrame(new_rains)])\n",
    "#         concat_max_temps = pd.concat([avg_max_temp, pd.DataFrame(new_max_temps)])\n",
    "#         concat_min_temps = pd.concat([avg_min_temp, pd.DataFrame(new_min_temps)])\n",
    "#         new_df = pd.DataFrame()\n",
    "#         new_df['Total Accumulated Radiation'] = concat_rads\n",
    "#         new_df['Total Accumulated Rain'] = concat_rain\n",
    "#         new_df['Annual Avg Max Temp'] = concat_max_temps\n",
    "#         new_df['Annual Avg Min Temp'] = concat_min_temps\n",
    "#         new_df['Class'] = index\n",
    "#         print(index)\n",
    "#         new_F.append(new_df)\n",
    "        \n",
    "#     return pd.concat(new_F)\n",
    "\n",
    "#new_data = synthesize_tabular_data(samples_out, no_stds, F, number_of_classes, len(data.index))\n",
    "#new_data = sits(data)\n",
    "fv_dfs = get_fv_dfs(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fc1d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_data = model.sample(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe53469",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.to_csv('data/SITS_synth5k_0215_SD2OH10_XGB_p7std.csv')\n",
    "new_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95ff074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get aggregate data\n",
    "targetDataLoc = '~/ctgan/data/annualized_OH_p5_std_1varPerYear_10.csv'\n",
    "#aggDataLoc = 'data/synth1_GA_only_063022.csv'\n",
    "\n",
    "aggDf = data #pd.read_csv(aggDataLoc)\n",
    "#aggDf = aggDf.drop(\"Unnamed: 0\",axis=1)\n",
    "targetDf = pd.read_csv(targetDataLoc)\n",
    "#targetDf = targetDf.drop(\"Unnamed: 0\",axis=1)'\n",
    "#aggDataLoc = 'data/synth1_GA_only_063022.csv'\n",
    "\n",
    "aggDf = new_data #pd.read_csv(aggDataLoc)\n",
    "#aggDf = aggDf.drop(\"Unnamed: 0\",axis=1)\n",
    "targetDf = pd.read_csv(targetDataLoc)\n",
    "#targetDf = targetDf.drop(\"Unnamed: 0\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1df971",
   "metadata": {},
   "outputs": [],
   "source": [
    "############## imports\n",
    "# general\n",
    "import statistics\n",
    "import datetime\n",
    "#from sklearn.externals import joblib # save and load models\n",
    "import random\n",
    "# data manipulation and exploration\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "## machine learning stuff\n",
    "# preprocessing\n",
    "from sklearn import preprocessing\n",
    "# feature selection\n",
    "from sklearn.feature_selection import SelectKBest, SelectPercentile\n",
    "from sklearn.feature_selection import f_regression\n",
    "# pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "# train/testing\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV, cross_val_score  \n",
    "# error calculations\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "# models\n",
    "from sklearn.linear_model import LinearRegression # linear regression\n",
    "from sklearn.linear_model import BayesianRidge #bayesisan ridge regression\n",
    "from sklearn.svm import SVC  # support vector machines classification\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor # import GaussianProcessRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor # k-nearest neightbors for regression\n",
    "from sklearn.neural_network import MLPRegressor # neural network for regression\n",
    "from sklearn.neural_network import MLPClassifier # neural network for classification\n",
    "from sklearn.tree import DecisionTreeRegressor # decision tree regressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor  # random forest regression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier # adaboost for classification\n",
    "import xgboost as xgb\n",
    "# saving models\n",
    "# from sklearn.externals import joblib\n",
    "import joblib\n",
    "\n",
    "# import the API\n",
    "APILoc = 'API/'\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, APILoc)\n",
    "\n",
    "from API import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079f1aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out the features that will not be used by the machine learning models\n",
    "\n",
    "# the features to keep:\n",
    "# xColumnsToKeep = [\"Julian Day\", \"Time Since Sown (Days)\", \"Time Since Last Harvest (Days)\", \"Total Radiation (MJ/m^2)\",\n",
    "#                \"Total Rainfall (mm)\", \"Avg Air Temp (C)\", \"Avg Min Temp (C)\", \"Avg Max Temp (C)\",\n",
    "#                  \"Avg Soil Moisture (%)\", \"Day Length (hrs)\"], \"Percent Cover (%)\"]\n",
    "\n",
    "#xColumnsToKeep = [\"Total Radiation (W/m^2)\",\"Total Rainfall (mm)\", \"Avg Max Temp (C)\", \"Avg Min Temp (C)\"]\n",
    "xColumnsToKeep = [\"Total Accumulated Radiation\",\"Total Accumulated Rain\", \"Annual Avg Max Temp\", \"Annual Avg Min Temp\"]\n",
    "\n",
    "#xColumnsToKeep = [\"Julian Day\", \"Time Since Sown (Days)\", \"Total Radiation (MJ/m^2)\", \"Total Rainfall (mm)\"]\n",
    "\n",
    "# the target to keep\n",
    "yColumnsToKeep = [\"Class\"]\n",
    "\n",
    "# get a dataframe containing the features and the targets\n",
    "xDf = aggDf[xColumnsToKeep]\n",
    "test_xDf = targetDf[xColumnsToKeep]\n",
    "yDf = aggDf[yColumnsToKeep]\n",
    "test_yDf = targetDf[yColumnsToKeep]\n",
    "\n",
    "# reset the index\n",
    "xDf = xDf.reset_index(drop=True)\n",
    "yDf = yDf.reset_index(drop=True)\n",
    "test_xDf = test_xDf.reset_index(drop=True)\n",
    "test_yDf = test_yDf.reset_index(drop=True)\n",
    "\n",
    "pd.set_option('display.max_rows', 2500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "xCols = list(xDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48fcfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide the warnings because training the neural network caues lots of warnings.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# make the parameter grids for sklearn's gridsearchcv\n",
    "rfParamGrid = {\n",
    "        'model__n_estimators': [5, 10, 25, 50, 100], # Number of estimators\n",
    "        'model__max_depth': [5, 10, 15, 20], # Maximum depth of the tree\n",
    "        'model__criterion': [\"gini\"]\n",
    "    }\n",
    "knnParamGrid ={\n",
    "        'model__n_neighbors':[2,5,10],\n",
    "        'model__weights': ['uniform', 'distance'],\n",
    "        'model__leaf_size': [5, 10, 30, 50]    \n",
    "    }\n",
    "svrParamGrid = {\n",
    "        'model__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "        'model__C': [0.1, 1.0, 5.0, 10.0],\n",
    "        'model__gamma': [\"scale\", \"auto\"],\n",
    "        'model__degree': [2,3,4,5]\n",
    "    }\n",
    "nnParamGrid = {\n",
    "        'model__hidden_layer_sizes':[(3), (5), (10), (3,3), (5,5), (7,7)],\n",
    "        'model__solver': ['sgd', 'adam'],\n",
    "        'model__learning_rate' : ['constant', 'invscaling', 'adaptive'],\n",
    "        'model__learning_rate_init': [0.1, 0.01, 0.001]      \n",
    "    }\n",
    "\n",
    "linRegParamGrid = {}\n",
    "\n",
    "bayesParamGrid={\n",
    "        'model__n_iter':[100,300,500]\n",
    "    }\n",
    "\n",
    "dtParamGrid = {\n",
    "    'model__criterion': ['gini'],\n",
    "    'model__max_depth': [5,10,25,50,100]\n",
    "    }\n",
    "\n",
    "xgbParamGrid = {}\n",
    "\n",
    "aModelList = [#(RandomForestClassifier(), rfParamGrid, \"rfTup.pkl\")]#,\n",
    "              #(KNeighborsRegressor(), knnParamGrid, \"knnTup.pkl\"),\n",
    "              #(SVC(), svrParamGrid, \"svrTup.pkl\")]#,\n",
    "             #(MLPClassifier(), nnParamGrid, \"nnTup.pkl\")]#,\n",
    "             #(LinearRegression(), linRegParamGrid, \"linRegTup.pkl\")]#,\n",
    "             #(BayesianRidge(), bayesParamGrid, \"bayesTup.pkl\"),\n",
    "             #(DecisionTreeClassifier(), dtParamGrid, \"dtTup.pkl\")]\n",
    "             (xgb.XGBClassifier(), xgbParamGrid, \"xgbTup.pkl\")]\n",
    "\n",
    "N = 10\n",
    "workingDir = 'working_dir'\n",
    "numFeatures = 4 # 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52261c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "saveMLResults(test_xDf, test_yDf, N, xDf, yDf, aModelList, workingDir, numFeatures, printResults=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ca6fda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
