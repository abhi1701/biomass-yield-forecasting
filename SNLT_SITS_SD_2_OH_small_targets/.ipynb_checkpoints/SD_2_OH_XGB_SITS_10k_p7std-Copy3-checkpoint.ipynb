{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43a6d61b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "\n",
    "data = genfromtxt('/home/jmvance/ctgan/data/annualized_SD_p5_std.csv', delimiter=',')\n",
    "for i in range(5):\n",
    "    print(data[i,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8ed05b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 classes\n",
      "767 rows\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'list' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m     cnt \u001b[38;5;241m=\u001b[39m cnt \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# F = [[]]\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m F \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF2\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(F)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;129m@jit\u001b[39m(nopython\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msynthesize_tabular_data\u001b[39m(F, samples_out, no_stds, no_classes, no_records):\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'list' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "# from sdv.tabular import CTGAN\n",
    "\n",
    "# model = CTGAN()\n",
    "# model.fit(data)\n",
    "import numba\n",
    "from numba import jit\n",
    "\n",
    "samples_out = 10000 # total number of samples/records to generate/synthesize\n",
    "no_stds = .7 # number of standard deviations within which synthesized values must fall\n",
    "#F = data[[\"Total Radiation (W/m^2)\",\"Total Rainfall (mm)\", \"Avg Max Temp (C)\", \"Avg Min Temp (C)\"]] # the input feature matrix\n",
    "number_of_classes = len(np.unique(data[:, 5])) - 1 # number of unique classes in input data\n",
    "print(str(number_of_classes) + \" classes\")\n",
    "data_len = np.shape(data)[0]\n",
    "print(str(data_len) + \" rows\" )\n",
    "F0 = [5] # array of the feature vectors dataframes, one per class\n",
    "F1 = [5]\n",
    "F2 = [5]\n",
    "\n",
    "cnt = 5\n",
    "for elem in np.nditer(data):\n",
    "    if(cnt % 5 == 0):\n",
    "        if (elem == 0):\n",
    "            F0.append(elem)\n",
    "            #print('added ' + str(elem) + \" no \" + str(cnt))\n",
    "        elif (elem == 1):\n",
    "            F1.append(elem)\n",
    "            #print('added ' + str(elem) + \" no \" + str(cnt))\n",
    "        elif (elem == 2):\n",
    "            F2.append(elem)\n",
    "            #print('added ' + str(elem) + \" no \" + str(cnt))\n",
    "    cnt = cnt + 1\n",
    "\n",
    "# F = [[]]\n",
    "F = np.concatenate((np.concatenate((F0, F1), F2)))\n",
    "print(F)\n",
    "    \n",
    "@jit(nopython=True)\n",
    "def synthesize_tabular_data(F, samples_out, no_stds, no_classes, no_records):\n",
    "    new_F = []\n",
    "    class_no = 0\n",
    "    for entry in F:\n",
    "        for cls in entry:\n",
    "            total_rad = cls[1]\n",
    "            print(total_rad)\n",
    "            mean_rad = total_rad.mean()\n",
    "            print(mean_rad)\n",
    "            std_rad = total_rad.std()\n",
    "            print(std_rad)\n",
    "#             total_rain = entry['Total Accumulated Rain']\n",
    "#             mean_rain = total_rain.mean()\n",
    "#             std_rain = total_rain.std()\n",
    "#             avg_max_temp = entry['Annual Avg Max Temp']\n",
    "#             mean_max_temp = avg_max_temp.mean()\n",
    "#             std_max_temp = avg_max_temp.std()\n",
    "#             avg_min_temp = entry['Annual Avg Min Temp']\n",
    "#             mean_min_temp = avg_min_temp.mean()\n",
    "#             std_min_temp = avg_min_temp.std()\n",
    "        \n",
    "        new_rads = []\n",
    "        new_rains = []\n",
    "        new_max_temps = []\n",
    "        new_min_temps = []\n",
    "        \n",
    "        # calculate potcii: percentage of this class in input\n",
    "        potcii = (len(entry)/no_records)\n",
    "        no_records_to_generate = round(potcii * samples_out)\n",
    "        \n",
    "        for i in range(no_records_to_generate):\n",
    "            new_rad = random.uniform(mean_rad - std_rad*no_stds, mean_rad + std_rad*no_stds)\n",
    "            new_rads.append(new_rad)\n",
    "            \n",
    "#             new_rain = random.uniform(mean_rain - std_rain*no_stds, mean_rain + std_rain*no_stds)\n",
    "#             new_rains.append(new_rain)\n",
    "        \n",
    "#             new_max_temp = random.uniform(mean_max_temp - std_max_temp*no_stds, mean_max_temp + std_max_temp*no_stds)\n",
    "#             new_max_temps.append(new_max_temp)\n",
    "            \n",
    "#             new_min_temp = random.uniform(mean_min_temp - std_min_temp*no_stds, mean_min_temp + std_min_temp*no_stds)\n",
    "#             new_min_temps.append(new_min_temp)\n",
    "            \n",
    "        concat_rads = np.concatenate([total_rad, new_rads])\n",
    "#         concat_rain = pd.concat([total_rain, pd.DataFrame(new_rains)])\n",
    "#         concat_max_temps = pd.concat([avg_max_temp, pd.DataFrame(new_max_temps)])\n",
    "#         concat_min_temps = pd.concat([avg_min_temp, pd.DataFrame(new_min_temps)])\n",
    "        new_ar = []\n",
    "        new_ar[1] = concat_rads\n",
    "#         new_ar['Total Accumulated Rain'] = concat_rain\n",
    "#         new_ar['Annual Avg Max Temp'] = concat_max_temps\n",
    "#         new_ar['Annual Avg Min Temp'] = concat_min_temps\n",
    "        new_ar[5] = class_no\n",
    "        class_no = class_no + 1\n",
    "        #print(index)\n",
    "        #new_F.append(new_df)\n",
    "        F[class_no].append(new_ar)\n",
    "        \n",
    "    return np.concatenate(np.concatenate(F[0],F[1]), F[2])\n",
    "\n",
    "new_data = synthesize_tabular_data(F, samples_out, no_stds, number_of_classes, data_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fc1d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_data = model.sample(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe53469",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.to_csv('data/SITS_synth5k_0215_SD2OH10_XGB_p7std_3.csv')\n",
    "new_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95ff074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get aggregate data\n",
    "targetDataLoc = '~/ctgan/data/annualized_OH_p5_std_1varPerYear_10.csv'\n",
    "#aggDataLoc = 'data/synth1_GA_only_063022.csv'\n",
    "\n",
    "aggDf = data #pd.read_csv(aggDataLoc)\n",
    "#aggDf = aggDf.drop(\"Unnamed: 0\",axis=1)\n",
    "targetDf = pd.read_csv(targetDataLoc)\n",
    "#targetDf = targetDf.drop(\"Unnamed: 0\",axis=1)'\n",
    "#aggDataLoc = 'data/synth1_GA_only_063022.csv'\n",
    "\n",
    "aggDf = new_data #pd.read_csv(aggDataLoc)\n",
    "#aggDf = aggDf.drop(\"Unnamed: 0\",axis=1)\n",
    "targetDf = pd.read_csv(targetDataLoc)\n",
    "#targetDf = targetDf.drop(\"Unnamed: 0\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1df971",
   "metadata": {},
   "outputs": [],
   "source": [
    "############## imports\n",
    "# general\n",
    "import statistics\n",
    "import datetime\n",
    "#from sklearn.externals import joblib # save and load models\n",
    "import random\n",
    "# data manipulation and exploration\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "## machine learning stuff\n",
    "# preprocessing\n",
    "from sklearn import preprocessing\n",
    "# feature selection\n",
    "from sklearn.feature_selection import SelectKBest, SelectPercentile\n",
    "from sklearn.feature_selection import f_regression\n",
    "# pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "# train/testing\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV, cross_val_score  \n",
    "# error calculations\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "# models\n",
    "from sklearn.linear_model import LinearRegression # linear regression\n",
    "from sklearn.linear_model import BayesianRidge #bayesisan ridge regression\n",
    "from sklearn.svm import SVC  # support vector machines classification\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor # import GaussianProcessRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor # k-nearest neightbors for regression\n",
    "from sklearn.neural_network import MLPRegressor # neural network for regression\n",
    "from sklearn.neural_network import MLPClassifier # neural network for classification\n",
    "from sklearn.tree import DecisionTreeRegressor # decision tree regressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor  # random forest regression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier # adaboost for classification\n",
    "import xgboost as xgb\n",
    "# saving models\n",
    "# from sklearn.externals import joblib\n",
    "import joblib\n",
    "\n",
    "# import the API\n",
    "APILoc = 'API/'\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, APILoc)\n",
    "\n",
    "from API import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079f1aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out the features that will not be used by the machine learning models\n",
    "\n",
    "# the features to keep:\n",
    "# xColumnsToKeep = [\"Julian Day\", \"Time Since Sown (Days)\", \"Time Since Last Harvest (Days)\", \"Total Radiation (MJ/m^2)\",\n",
    "#                \"Total Rainfall (mm)\", \"Avg Air Temp (C)\", \"Avg Min Temp (C)\", \"Avg Max Temp (C)\",\n",
    "#                  \"Avg Soil Moisture (%)\", \"Day Length (hrs)\"], \"Percent Cover (%)\"]\n",
    "\n",
    "#xColumnsToKeep = [\"Total Radiation (W/m^2)\",\"Total Rainfall (mm)\", \"Avg Max Temp (C)\", \"Avg Min Temp (C)\"]\n",
    "xColumnsToKeep = [\"Total Accumulated Radiation\",\"Total Accumulated Rain\", \"Annual Avg Max Temp\", \"Annual Avg Min Temp\"]\n",
    "\n",
    "#xColumnsToKeep = [\"Julian Day\", \"Time Since Sown (Days)\", \"Total Radiation (MJ/m^2)\", \"Total Rainfall (mm)\"]\n",
    "\n",
    "# the target to keep\n",
    "yColumnsToKeep = [\"Class\"]\n",
    "\n",
    "# get a dataframe containing the features and the targets\n",
    "xDf = aggDf[xColumnsToKeep]\n",
    "test_xDf = targetDf[xColumnsToKeep]\n",
    "yDf = aggDf[yColumnsToKeep]\n",
    "test_yDf = targetDf[yColumnsToKeep]\n",
    "\n",
    "# reset the index\n",
    "xDf = xDf.reset_index(drop=True)\n",
    "yDf = yDf.reset_index(drop=True)\n",
    "test_xDf = test_xDf.reset_index(drop=True)\n",
    "test_yDf = test_yDf.reset_index(drop=True)\n",
    "\n",
    "pd.set_option('display.max_rows', 2500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "xCols = list(xDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48fcfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide the warnings because training the neural network caues lots of warnings.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# make the parameter grids for sklearn's gridsearchcv\n",
    "rfParamGrid = {\n",
    "        'model__n_estimators': [5, 10, 25, 50, 100], # Number of estimators\n",
    "        'model__max_depth': [5, 10, 15, 20], # Maximum depth of the tree\n",
    "        'model__criterion': [\"gini\"]\n",
    "    }\n",
    "knnParamGrid ={\n",
    "        'model__n_neighbors':[2,5,10],\n",
    "        'model__weights': ['uniform', 'distance'],\n",
    "        'model__leaf_size': [5, 10, 30, 50]    \n",
    "    }\n",
    "svrParamGrid = {\n",
    "        'model__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "        'model__C': [0.1, 1.0, 5.0, 10.0],\n",
    "        'model__gamma': [\"scale\", \"auto\"],\n",
    "        'model__degree': [2,3,4,5]\n",
    "    }\n",
    "nnParamGrid = {\n",
    "        'model__hidden_layer_sizes':[(3), (5), (10), (3,3), (5,5), (7,7)],\n",
    "        'model__solver': ['sgd', 'adam'],\n",
    "        'model__learning_rate' : ['constant', 'invscaling', 'adaptive'],\n",
    "        'model__learning_rate_init': [0.1, 0.01, 0.001]      \n",
    "    }\n",
    "\n",
    "linRegParamGrid = {}\n",
    "\n",
    "bayesParamGrid={\n",
    "        'model__n_iter':[100,300,500]\n",
    "    }\n",
    "\n",
    "dtParamGrid = {\n",
    "    'model__criterion': ['gini'],\n",
    "    'model__max_depth': [5,10,25,50,100]\n",
    "    }\n",
    "\n",
    "xgbParamGrid = {}\n",
    "\n",
    "aModelList = [#(RandomForestClassifier(), rfParamGrid, \"rfTup.pkl\")]#,\n",
    "              #(KNeighborsRegressor(), knnParamGrid, \"knnTup.pkl\"),\n",
    "              #(SVC(), svrParamGrid, \"svrTup.pkl\")]#,\n",
    "             #(MLPClassifier(), nnParamGrid, \"nnTup.pkl\")]#,\n",
    "             #(LinearRegression(), linRegParamGrid, \"linRegTup.pkl\")]#,\n",
    "             #(BayesianRidge(), bayesParamGrid, \"bayesTup.pkl\"),\n",
    "             #(DecisionTreeClassifier(), dtParamGrid, \"dtTup.pkl\")]\n",
    "             (xgb.XGBClassifier(), xgbParamGrid, \"xgbTup.pkl\")]\n",
    "\n",
    "N = 10\n",
    "workingDir = 'working_dir'\n",
    "numFeatures = 4 # 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52261c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "saveMLResults(test_xDf, test_yDf, N, xDf, yDf, aModelList, workingDir, numFeatures, printResults=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ca6fda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
